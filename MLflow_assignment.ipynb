{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "K8O-vKTS3tMk",
        "outputId": "0bccba64-0ec1-4ce3-a2b2-aefc061b90f9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-495297892.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-495297892.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    mlflow>=2.10.0\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Create a new directory for your project, create requirements.txt, and install packages\n",
        "!mkdir mlflow-assignment-1 && cd mlflow-assignment-1 && cat > requirements.txt << EOF\n",
        "mlflow>=2.10.0\n",
        "scikit-learn>=1.2.0\n",
        "pandas>=1.5.0\n",
        "numpy>=1.21.0\n",
        "matplotlib>=3.5.0\n",
        "seaborn>=0.11.0\n",
        "plotly>=5.13.0\n",
        "jupyter>=1.0.0\n",
        "notebook>=6.5.0\n",
        "boto3>=1.26.0  # For AWS integration (optional)\n",
        "azureml-core>=1.50.0  # For Azure integration (optional)\n",
        "google-cloud-aiplatform>=1.25.0  # For GCP integration (optional)\n",
        "EOF\n",
        "!pip install -r mlflow-assignment-1/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ad59fc9"
      },
      "source": [
        "# Create a new directory for your project\n",
        "!mkdir mlflow-assignment-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963c25ac",
        "outputId": "1b2cdda9-d697-45ff-cd57-147717f92137"
      },
      "source": [
        "%%writefile mlflow-assignment-1/requirements.txt\n",
        "mlflow>=2.10.0\n",
        "scikit-learn>=1.2.0\n",
        "pandas>=1.5.0\n",
        "numpy>=1.21.0\n",
        "matplotlib>=3.5.0\n",
        "seaborn>=0.11.0\n",
        "plotly>=5.13.0\n",
        "jupyter>=1.0.0\n",
        "notebook>=6.5.0\n",
        "boto3>=1.26.0  # For AWS integration (optional)\n",
        "azureml-core>=1.50.0  # For Azure integration (optional)\n",
        "google-cloud-aiplatform>=1.25.0  # For GCP integration (optional)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fce76f",
        "outputId": "b2d6fbf4-fb97-44e4-986e-4da6f59f6cb2"
      },
      "source": [
        "# Install all required packages\n",
        "!pip install -r mlflow-assignment-1/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow>=2.10.0 (from -r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: plotly>=5.13.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 7)) (5.24.1)\n",
            "Collecting jupyter>=1.0.0 (from -r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: notebook>=6.5.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 9)) (6.5.7)\n",
            "Collecting boto3>=1.26.0 (from -r mlflow-assignment-1/requirements.txt (line 10))\n",
            "  Downloading boto3-1.40.61-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting azureml-core>=1.50.0 (from -r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azureml_core-1.60.0.post1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from -r mlflow-assignment-1/requirements.txt (line 12)) (1.122.0)\n",
            "Collecting mlflow-skinny==3.5.1 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading mlflow_skinny-3.5.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.5.1 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading mlflow_tracing-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<22,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.16.2)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.0.44)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (8.3.0)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading databricks_sdk-0.70.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.120.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.11.10)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.38.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.0->-r mlflow-assignment-1/requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.0->-r mlflow-assignment-1/requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r mlflow-assignment-1/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r mlflow-assignment-1/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r mlflow-assignment-1/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->-r mlflow-assignment-1/requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.13.0->-r mlflow-assignment-1/requirements.txt (line 7)) (8.5.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading jupyterlab-4.4.10-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (6.5.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (26.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (25.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (5.9.1)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (7.4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (5.10.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (1.3.3)\n",
            "Collecting botocore<1.41.0,>=1.40.61 (from boto3>=1.26.0->-r mlflow-assignment-1/requirements.txt (line 10))\n",
            "  Downloading botocore-1.40.61-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.0->-r mlflow-assignment-1/requirements.txt (line 10))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.26.0->-r mlflow-assignment-1/requirements.txt (line 10))\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting backports.tempfile (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pathspec<1.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting msal<2.0.0,>=1.15.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions<=2.0.0,>=0.3.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting knack<0.13.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading knack-0.12.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting azure-core<2.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pkginfo (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting argcomplete<4 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting humanfriendly<11.0,>=4.7 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting paramiko<4.0.0,>=2.0.8 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting azure-mgmt-resource<=24.0.0,>=15.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_resource-24.0.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerregistry<14,>=8.2.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_containerregistry-13.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting azure-mgmt-storage<=23.0.0,>=16.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_storage-23.0.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting azure-mgmt-keyvault<12.0.0,>=0.40.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_keyvault-11.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting azure-mgmt-authorization<5,>=0.40.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_authorization-4.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting azure-mgmt-network<=29.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_network-29.0.0-py3-none-any.whl.metadata (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-graphrbac<1.0.0,>=0.40.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_graphrbac-0.61.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-common<2.0.0,>=1.1.12 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting msrest<=0.7.1,>=0.5.1 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting msrestazure<=0.7,>=0.4.33 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading msrestazure-0.6.4.post1-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: urllib3<3.0.0,>1.26.17 in /usr/local/lib/python3.12/dist-packages (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (2.5.0)\n",
            "Collecting ndg-httpsclient<=0.5.1 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: SecretStorage<4.0.0 in /usr/local/lib/python3.12/dist-packages (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: jsonpickle<5.0.0 in /usr/local/lib/python3.12/dist-packages (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (4.1.1)\n",
            "Collecting contextlib2<22.0.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: PyJWT<3.0.0 in /usr/local/lib/python3.12/dist-packages (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (2.10.1)\n",
            "Collecting adal<=1.2.7,>=1.2.0 (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pyopenssl<26.0.0 in /usr/local/lib/python3.12/dist-packages (from azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (24.2.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.27.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.26.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.46.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (0.17.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.3.10)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-mgmt-authorization<5,>=0.40.0->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-mgmt-core<2.0.0,>=1.3.2 (from azure-mgmt-authorization<5,>=0.40.0->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.71.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (4.11.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (15.0.1)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1))\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client<8,>=5.3.4->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.6.1->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (4.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from knack<0.13.0->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from knack<0.13.0->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from msrest<=0.7.1,>=0.5.1->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (2025.10.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from msrest<=0.7.1,>=0.5.1->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from msrestazure<=0.7,>=0.4.33->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (4.25.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ndg-httpsclient<=0.5.1->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (0.6.1)\n",
            "Collecting bcrypt>=3.2 (from paramiko<4.0.0,>=2.0.8->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting pynacl>=1.5 (from paramiko<4.0.0,>=2.0.8->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (1.7.1)\n",
            "Requirement already satisfied: jeepney>=0.6 in /usr/local/lib/python3.12/dist-packages (from SecretStorage<4.0.0->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.2.4)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (25.1.0)\n",
            "Collecting backports.weakref (from backports.tempfile->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11))\n",
            "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (3.0.15)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (3.0.52)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (75.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (2.23)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.48.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform>=1.25.0->-r mlflow-assignment-1/requirements.txt (line 12)) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (3.23.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.5.0->-r mlflow-assignment-1/requirements.txt (line 9)) (0.28.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (0.58b0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.2.14)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core>=1.50.0->-r mlflow-assignment-1/requirements.txt (line 11)) (3.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (2.8)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow>=2.10.0->-r mlflow-assignment-1/requirements.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.8.5)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r mlflow-assignment-1/requirements.txt (line 8)) (1.4.0)\n",
            "Downloading mlflow-3.5.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.5.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.5.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading boto3-1.40.61-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azureml_core-1.60.0.post1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_graphrbac-0.61.2-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_authorization-4.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerregistry-13.0.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_keyvault-11.0.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.8/308.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_network-29.0.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.0/608.0 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_resource-24.0.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_storage-23.0.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.5/286.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.61-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading knack-0.12.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msrestazure-0.6.4.post1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
            "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
            "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
            "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.70.0-py3-none-any.whl (752 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: backports.weakref, azure-common, pkginfo, pathspec, json5, jmespath, jedi, isodate, humanfriendly, gunicorn, graphql-core, contextlib2, bcrypt, backports.tempfile, async-lru, argcomplete, pynacl, knack, graphql-relay, docker, botocore, azure-core, s3transfer, paramiko, msrest, graphene, Flask-CORS, databricks-sdk, azure-mgmt-core, adal, ndg-httpsclient, msrestazure, msal, boto3, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-network, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, msal-extensions, mlflow-tracing, mlflow-skinny, azure-graphrbac, mlflow, azureml-core, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "Successfully installed Flask-CORS-6.0.1 adal-1.2.7 argcomplete-3.6.3 async-lru-2.0.5 azure-common-1.1.28 azure-core-1.36.0 azure-graphrbac-0.61.2 azure-mgmt-authorization-4.0.0 azure-mgmt-containerregistry-13.0.0 azure-mgmt-core-1.6.0 azure-mgmt-keyvault-11.0.0 azure-mgmt-network-29.0.0 azure-mgmt-resource-24.0.0 azure-mgmt-storage-23.0.0 azureml-core-1.60.0.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-5.0.0 boto3-1.40.61 botocore-1.40.61 contextlib2-21.6.0 databricks-sdk-0.70.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 humanfriendly-10.0 isodate-0.7.2 jedi-0.19.2 jmespath-1.0.1 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.4.10 jupyterlab-server-2.28.0 knack-0.12.0 mlflow-3.5.1 mlflow-skinny-3.5.1 mlflow-tracing-3.5.1 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 msrestazure-0.6.4.post1 ndg-httpsclient-0.5.1 paramiko-3.5.1 pathspec-0.12.1 pkginfo-1.12.1.2 pynacl-1.6.0 s3transfer-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_preparation.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and prepare the wine quality dataset\"\"\"\n",
        "        print(\"Loading Wine Quality Dataset...\")\n",
        "\n",
        "        # Method 1: Using fetch_openml (recommended)\n",
        "        try:\n",
        "            wine = fetch_openml(name='wine-quality-red', version=1, as_frame=True)\n",
        "            df = wine.frame\n",
        "            df['quality'] = df['quality'].astype(int)\n",
        "        except:\n",
        "            # Method 2: Download from URL\n",
        "            url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "            df = pd.read_csv(url, delimiter=';')\n",
        "\n",
        "        print(f\"Dataset loaded with {df.shape[0]} samples and {df.shape[1]} features\")\n",
        "        return df\n",
        "\n",
        "    def explore_data(self, df):\n",
        "        \"\"\"Perform basic data exploration\"\"\"\n",
        "        print(\"\\n=== Data Exploration ===\")\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(f\"\\nColumn types:\\n{df.dtypes}\")\n",
        "        print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "        print(f\"\\nDataset description:\\n{df.describe()}\")\n",
        "\n",
        "        # Check target distribution\n",
        "        print(f\"\\nTarget distribution (quality):\\n{df['quality'].value_counts().sort_index()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"Preprocess the data for modeling\"\"\"\n",
        "        print(\"\\n=== Data Preprocessing ===\")\n",
        "\n",
        "        # Create a binary classification problem (good wine vs bad wine)\n",
        "        df['wine_quality'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
        "\n",
        "        # Features and target\n",
        "        X = df.drop(['quality', 'wine_quality'], axis=1)\n",
        "        y = df['wine_quality']\n",
        "\n",
        "        print(f\"Features: {X.columns.tolist()}\")\n",
        "        print(f\"Target distribution: {y.value_counts()}\")\n",
        "        print(f\"Positive class ratio: {y.mean():.3f}\")\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Scale numerical features\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        print(f\"Training set: {X_train_scaled.shape}\")\n",
        "        print(f\"Test set: {X_test_scaled.shape}\")\n",
        "\n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df = preprocessor.load_data()\n",
        "    df = preprocessor.explore_data(df)\n",
        "    X_train, X_test, y_train, y_test, feature_names = preprocessor.preprocess_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueR-wExl7YHb",
        "outputId": "7c3d5089-ed5d-4db2-ff25-5dd59df46114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wine Quality Dataset...\n",
            "Dataset loaded with 1599 samples and 12 features\n",
            "\n",
            "=== Data Exploration ===\n",
            "Dataset shape: (1599, 12)\n",
            "\n",
            "Column types:\n",
            "fixed acidity           float64\n",
            "volatile acidity        float64\n",
            "citric acid             float64\n",
            "residual sugar          float64\n",
            "chlorides               float64\n",
            "free sulfur dioxide     float64\n",
            "total sulfur dioxide    float64\n",
            "density                 float64\n",
            "pH                      float64\n",
            "sulphates               float64\n",
            "alcohol                 float64\n",
            "quality                   int64\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "fixed acidity           0\n",
            "volatile acidity        0\n",
            "citric acid             0\n",
            "residual sugar          0\n",
            "chlorides               0\n",
            "free sulfur dioxide     0\n",
            "total sulfur dioxide    0\n",
            "density                 0\n",
            "pH                      0\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "quality                 0\n",
            "dtype: int64\n",
            "\n",
            "Dataset description:\n",
            "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
            "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
            "mean        8.319637          0.527821     0.270976        2.538806   \n",
            "std         1.741096          0.179060     0.194801        1.409928   \n",
            "min         4.600000          0.120000     0.000000        0.900000   \n",
            "25%         7.100000          0.390000     0.090000        1.900000   \n",
            "50%         7.900000          0.520000     0.260000        2.200000   \n",
            "75%         9.200000          0.640000     0.420000        2.600000   \n",
            "max        15.900000          1.580000     1.000000       15.500000   \n",
            "\n",
            "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
            "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
            "mean      0.087467            15.874922             46.467792     0.996747   \n",
            "std       0.047065            10.460157             32.895324     0.001887   \n",
            "min       0.012000             1.000000              6.000000     0.990070   \n",
            "25%       0.070000             7.000000             22.000000     0.995600   \n",
            "50%       0.079000            14.000000             38.000000     0.996750   \n",
            "75%       0.090000            21.000000             62.000000     0.997835   \n",
            "max       0.611000            72.000000            289.000000     1.003690   \n",
            "\n",
            "                pH    sulphates      alcohol      quality  \n",
            "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
            "mean      3.311113     0.658149    10.422983     5.636023  \n",
            "std       0.154386     0.169507     1.065668     0.807569  \n",
            "min       2.740000     0.330000     8.400000     3.000000  \n",
            "25%       3.210000     0.550000     9.500000     5.000000  \n",
            "50%       3.310000     0.620000    10.200000     6.000000  \n",
            "75%       3.400000     0.730000    11.100000     6.000000  \n",
            "max       4.010000     2.000000    14.900000     8.000000  \n",
            "\n",
            "Target distribution (quality):\n",
            "quality\n",
            "3     10\n",
            "4     53\n",
            "5    681\n",
            "6    638\n",
            "7    199\n",
            "8     18\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Data Preprocessing ===\n",
            "Features: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
            "Target distribution: wine_quality\n",
            "0    1382\n",
            "1     217\n",
            "Name: count, dtype: int64\n",
            "Positive class ratio: 0.136\n",
            "Training set: (1279, 11)\n",
            "Test set: (320, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mlflow_setup.py\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import tempfile\n",
        "\n",
        "class MLflowSetup:\n",
        "    def __init__(self, experiment_name=\"Wine-Quality-Classification\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.setup_tracking()\n",
        "\n",
        "    def setup_tracking(self):\n",
        "        \"\"\"Setup MLflow tracking with multiple backend options\"\"\"\n",
        "\n",
        "        # Option 1: Local File System (Default)\n",
        "        tracking_uri = \"file:///./mlruns\"\n",
        "\n",
        "        # Option 2: SQLite Backend (Uncomment to use)\n",
        "        # tracking_uri = \"sqlite:///mlflow.db\"\n",
        "\n",
        "        # Option 3: Remote Server (Uncomment and modify for your setup)\n",
        "        # tracking_uri = \"http://your-mlflow-server:5000\"\n",
        "\n",
        "        # Option 4: AWS S3 Backend (Uncomment for AWS deployment)\n",
        "        # tracking_uri = \"file:///./mlruns\"\n",
        "        # os.environ['AWS_ACCESS_KEY_ID'] = 'your-access-key'\n",
        "        # os.environ['AWS_SECRET_ACCESS_KEY'] = 'your-secret-key'\n",
        "        # os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https://s3.amazonaws.com'\n",
        "\n",
        "        mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "        # Set experiment\n",
        "        mlflow.set_experiment(self.experiment_name)\n",
        "\n",
        "        print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "        print(f\"Experiment: {self.experiment_name}\")\n",
        "\n",
        "    def create_run_name(self, model_type, run_description):\n",
        "        \"\"\"Create meaningful run names\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        return f\"{model_type}_{timestamp}_{run_description}\"\n",
        "\n",
        "# Initialize MLflow setup\n",
        "mlflow_setup = MLflowSetup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL031XY68CfB",
        "outputId": "354c6ecb-9a1d-40c4-e426-58dfcf33f9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/10/29 07:48:31 INFO mlflow.tracking.fluent: Experiment with name 'Wine-Quality-Classification' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow tracking URI: file:///./mlruns\n",
            "Experiment: Wine-Quality-Classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_training.py\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import json\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test, feature_names):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "\n",
        "    def evaluate_model(self, model, model_type):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        # Predictions\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred_proba = model.predict_proba(self.X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "            'precision': precision_score(self.y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(self.y_test, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(self.y_test, y_pred, zero_division=0),\n",
        "        }\n",
        "\n",
        "        if y_pred_proba is not None:\n",
        "            metrics['roc_auc'] = roc_auc_score(self.y_test, y_pred_proba)\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='f1')\n",
        "        metrics['cv_f1_mean'] = cv_scores.mean()\n",
        "        metrics['cv_f1_std'] = cv_scores.std()\n",
        "\n",
        "        return metrics, y_pred, y_pred_proba\n",
        "\n",
        "    def create_plots(self, model, y_pred, y_pred_proba, model_type):\n",
        "        \"\"\"Create evaluation plots\"\"\"\n",
        "        plots = {}\n",
        "\n",
        "        # Confusion Matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(self.y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix - {model_type}')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "\n",
        "        cm_path = tempfile.mktemp(suffix='_cm.png')\n",
        "        plt.savefig(cm_path)\n",
        "        plt.close()\n",
        "        plots['confusion_matrix'] = cm_path\n",
        "\n",
        "        # Feature Importance (for tree-based models)\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            feature_imp = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            sns.barplot(data=feature_imp.head(10), x='importance', y='feature')\n",
        "            plt.title(f'Feature Importance - {model_type}')\n",
        "\n",
        "            fi_path = tempfile.mktemp(suffix='_feature_importance.png')\n",
        "            plt.savefig(fi_path)\n",
        "            plt.close()\n",
        "            plots['feature_importance'] = fi_path\n",
        "\n",
        "        return plots\n",
        "\n",
        "    def train_random_forest(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Random Forest Classifier with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"RandomForest\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 10,\n",
        "                'min_samples_split': 2,\n",
        "                'min_samples_leaf': 1,\n",
        "                'random_state': 42\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Log dataset info\n",
        "            mlflow.log_param(\"dataset_shape\", f\"{self.X_train.shape}\")\n",
        "            mlflow.log_param(\"feature_count\", len(self.feature_names))\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Random Forest...\")\n",
        "            model = RandomForestClassifier(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"RandomForest\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Create and log plots\n",
        "            plots = self.create_plots(model, y_pred, y_pred_proba, \"RandomForest\")\n",
        "            for plot_name, plot_path in plots.items():\n",
        "                mlflow.log_artifact(plot_path, \"plots\")\n",
        "                os.remove(plot_path)  # Clean up temp file\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"random_forest_model\",\n",
        "                registered_model_name=\"RandomForest_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            # Log feature names as artifact\n",
        "            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
        "                json.dump({'feature_names': self.feature_names.tolist()}, f)\n",
        "                mlflow.log_artifact(f.name, \"metadata\")\n",
        "                os.remove(f.name)\n",
        "\n",
        "            print(f\"Random Forest completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model\n",
        "\n",
        "    def train_gradient_boosting(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Gradient Boosting Classifier with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"GradientBoosting\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'n_estimators': 100,\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 3,\n",
        "                'random_state': 42\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Gradient Boosting...\")\n",
        "            model = GradientBoostingClassifier(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"GradientBoosting\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Create and log plots\n",
        "            plots = self.create_plots(model, y_pred, y_pred_proba, \"GradientBoosting\")\n",
        "            for plot_name, plot_path in plots.items():\n",
        "                mlflow.log_artifact(plot_path, \"plots\")\n",
        "                os.remove(plot_path)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"gradient_boosting_model\",\n",
        "                registered_model_name=\"GradientBoosting_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            print(f\"Gradient Boosting completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model\n",
        "\n",
        "    def train_logistic_regression(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Logistic Regression with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"LogisticRegression\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'C': 1.0,\n",
        "                'max_iter': 1000,\n",
        "                'random_state': 42,\n",
        "                'solver': 'liblinear'\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Logistic Regression...\")\n",
        "            model = LogisticRegression(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"LogisticRegression\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"logistic_regression_model\",\n",
        "                registered_model_name=\"LogisticRegression_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            print(f\"Logistic Regression completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model"
      ],
      "metadata": {
        "id": "SKieGyHV8ItX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter_tuning.py\n",
        "import mlflow\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import pandas as pd\n",
        "\n",
        "class HyperparameterTuner:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test, feature_names):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "        self.trainer = ModelTrainer(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    def tune_random_forest(self):\n",
        "        \"\"\"Perform hyperparameter tuning for Random Forest\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"RandomForest\", \"hyperparameter_tuning\")\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameter grid\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [5, 10, 15, None],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "\n",
        "            # Log tuning parameters\n",
        "            mlflow.log_param(\"tuning_method\", \"GridSearchCV\")\n",
        "            mlflow.log_param(\"param_grid\", str(param_grid))\n",
        "            mlflow.log_param(\"cv_folds\", 5)\n",
        "\n",
        "            # Perform grid search\n",
        "            print(\"Performing GridSearch for Random Forest...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                RandomForestClassifier(random_state=42),\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='f1',\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Log best parameters\n",
        "            best_params = grid_search.best_params_\n",
        "            for param, value in best_params.items():\n",
        "                mlflow.log_param(f\"best_{param}\", value)\n",
        "\n",
        "            mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            best_model = grid_search.best_estimator_\n",
        "            metrics, y_pred, y_pred_proba = self.trainer.evaluate_model(best_model, \"RandomForest_Tuned\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                best_model,\n",
        "                \"random_forest_tuned_model\",\n",
        "                registered_model_name=\"RandomForest_Wine_Quality_Tuned\"\n",
        "            )\n",
        "\n",
        "            print(f\"Random Forest Tuning completed - Best F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, best_model, grid_search.best_params_\n",
        "\n",
        "    def tune_gradient_boosting(self):\n",
        "        \"\"\"Perform hyperparameter tuning for Gradient Boosting\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"GradientBoosting\", \"hyperparameter_tuning\")\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameter grid\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 4, 5],\n",
        "                'subsample': [0.8, 0.9, 1.0]\n",
        "            }\n",
        "\n",
        "            # Log tuning parameters\n",
        "            mlflow.log_param(\"tuning_method\", \"GridSearchCV\")\n",
        "            mlflow.log_param(\"param_grid\", str(param_grid))\n",
        "\n",
        "            # Perform grid search\n",
        "            print(\"Performing GridSearch for Gradient Boosting...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                GradientBoostingClassifier(random_state=42),\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='f1',\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Log best parameters\n",
        "            best_params = grid_search.best_params_\n",
        "            for param, value in best_params.items():\n",
        "                mlflow.log_param(f\"best_{param}\", value)\n",
        "\n",
        "            mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            best_model = grid_search.best_estimator_\n",
        "            metrics, y_pred, y_pred_proba = self.trainer.evaluate_model(best_model, \"GradientBoosting_Tuned\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                best_model,\n",
        "                \"gradient_boosting_tuned_model\",\n",
        "                registered_model_name=\"GradientBoosting_Wine_Quality_Tuned\"\n",
        "            )\n",
        "\n",
        "            print(f\"Gradient Boosting Tuning completed - Best F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, best_model, grid_search.best_params_"
      ],
      "metadata": {
        "id": "_b7xNipJ8OXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_selection.py\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class ModelSelector:\n",
        "    def __init__(self):\n",
        "        self.client = MlflowClient()\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "\n",
        "    def get_all_runs(self):\n",
        "        \"\"\"Retrieve all runs from the current experiment\"\"\"\n",
        "        experiment = mlflow.get_experiment_by_name(self.mlflow_setup.experiment_name)\n",
        "        runs = self.client.search_runs(experiment.experiment_id)\n",
        "\n",
        "        run_data = []\n",
        "        for run in runs:\n",
        "            run_info = {\n",
        "                'run_id': run.info.run_id,\n",
        "                'run_name': run.data.tags.get('mlflow.runName', ''),\n",
        "                'model_type': run.data.tags.get('mlflow.runName', '').split('_')[0],\n",
        "                'status': run.info.status,\n",
        "                'start_time': run.info.start_time,\n",
        "            }\n",
        "\n",
        "            # Add parameters\n",
        "            for key, value in run.data.params.items():\n",
        "                run_info[f'param_{key}'] = value\n",
        "\n",
        "            # Add metrics\n",
        "            for key, value in run.data.metrics.items():\n",
        "                run_info[f'metric_{key}'] = value\n",
        "\n",
        "            run_data.append(run_info)\n",
        "\n",
        "        return pd.DataFrame(run_data)\n",
        "\n",
        "    def select_best_model(self, primary_metric='metric_f1_score', secondary_metric='metric_roc_auc'):\n",
        "        \"\"\"Select the best model based on specified metrics\"\"\"\n",
        "        runs_df = self.get_all_runs()\n",
        "\n",
        "        if runs_df.empty:\n",
        "            print(\"No runs found in the experiment\")\n",
        "            return None\n",
        "\n",
        "        # Filter completed runs\n",
        "        completed_runs = runs_df[runs_df['status'] == 'FINISHED']\n",
        "\n",
        "        if completed_runs.empty:\n",
        "            print(\"No completed runs found\")\n",
        "            return None\n",
        "\n",
        "        # Sort by primary and secondary metrics\n",
        "        best_run = completed_runs.sort_values(\n",
        "            [primary_metric, secondary_metric],\n",
        "            ascending=[False, False]\n",
        "        ).iloc[0]\n",
        "\n",
        "        print(\"=== BEST MODEL SELECTION RESULTS ===\")\n",
        "        print(f\"Best Run ID: {best_run['run_id']}\")\n",
        "        print(f\"Best Run Name: {best_run['run_name']}\")\n",
        "        print(f\"Primary Metric ({primary_metric}): {best_run[primary_metric]:.4f}\")\n",
        "        print(f\"Secondary Metric ({secondary_metric}): {best_run.get(secondary_metric, 'N/A')}\")\n",
        "\n",
        "        # Display comparison table\n",
        "        comparison_df = completed_runs.groupby('model_type').agg({\n",
        "            'metric_accuracy': 'mean',\n",
        "            'metric_precision': 'mean',\n",
        "            'metric_recall': 'mean',\n",
        "            'metric_f1_score': 'mean',\n",
        "            'metric_roc_auc': 'mean'\n",
        "        }).round(4)\n",
        "\n",
        "        print(\"\\n=== MODEL COMPARISON ===\")\n",
        "        print(comparison_df)\n",
        "\n",
        "        return best_run"
      ],
      "metadata": {
        "id": "m1X4tYmO8U7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_registry.py\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import time\n",
        "\n",
        "class ModelRegistryManager:\n",
        "    def __init__(self):\n",
        "        self.client = MlflowClient()\n",
        "\n",
        "    def register_best_model(self, run_id, model_name=\"Best_Wine_Quality_Model\"):\n",
        "        \"\"\"Register the best model in MLflow Model Registry\"\"\"\n",
        "\n",
        "        # Construct model URI\n",
        "        model_uri = f\"runs:/{run_id}/model\"\n",
        "\n",
        "        try:\n",
        "            # Register the model\n",
        "            print(f\"Registering model from run {run_id}...\")\n",
        "            mv = mlflow.register_model(model_uri, model_name)\n",
        "\n",
        "            print(f\"Model registered successfully!\")\n",
        "            print(f\"Model Name: {mv.name}\")\n",
        "            print(f\"Model Version: {mv.version}\")\n",
        "            print(f\"Current Stage: {mv.current_stage}\")\n",
        "\n",
        "            return mv\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error registering model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def transition_model_stage(self, model_name, version, stage):\n",
        "        \"\"\"Transition model to different stages (Staging → Production)\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Transition model stage\n",
        "            self.client.transition_model_version_stage(\n",
        "                name=model_name,\n",
        "                version=version,\n",
        "                stage=stage\n",
        "            )\n",
        "\n",
        "            print(f\"Model {model_name} version {version} transitioned to {stage} stage\")\n",
        "\n",
        "            # Wait for transition to complete\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Get updated model version\n",
        "            mv = self.client.get_model_version(model_name, version)\n",
        "            print(f\"Current stage: {mv.current_stage}\")\n",
        "\n",
        "            return mv\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transitioning model stage: {e}\")\n",
        "            return None\n",
        "\n",
        "    def list_registered_models(self):\n",
        "        \"\"\"List all registered models\"\"\"\n",
        "        models = self.client.search_registered_models()\n",
        "\n",
        "        print(\"=== REGISTERED MODELS ===\")\n",
        "        for model in models:\n",
        "            print(f\"Model: {model.name}\")\n",
        "            for version in model.latest_versions:\n",
        "                print(f\"  Version {version.version}: {version.current_stage}\")\n",
        "\n",
        "    def archive_old_versions(self, model_name, keep_versions=3):\n",
        "        \"\"\"Archive old model versions to keep registry clean\"\"\"\n",
        "        try:\n",
        "            # Get all versions\n",
        "            versions = self.client.search_model_versions(f\"name='{model_name}'\")\n",
        "\n",
        "            # Sort by version number and get old versions\n",
        "            sorted_versions = sorted(versions, key=lambda x: x.version, reverse=True)\n",
        "            old_versions = sorted_versions[keep_versions:]\n",
        "\n",
        "            for version in old_versions:\n",
        "                if version.current_stage == \"None\":\n",
        "                    self.client.transition_model_version_stage(\n",
        "                        name=model_name,\n",
        "                        version=version.version,\n",
        "                        stage=\"Archived\"\n",
        "                    )\n",
        "                    print(f\"Archived version {version.version}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error archiving old versions: {e}\")"
      ],
      "metadata": {
        "id": "5u9WK7rq8eew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "import mlflow\n",
        "from data_preparation import DataPreprocessor\n",
        "from model_training import ModelTrainer\n",
        "from hyperparameter_tuning import HyperparameterTuner\n",
        "from model_selection import ModelSelector\n",
        "from model_registry import ModelRegistryManager\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    print(\"🚀 Starting MLOps Assignment 1 - MLflow Experiment Tracking\")\n",
        "\n",
        "    # Step 1: Data Preparation\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: Data Preparation\")\n",
        "    print(\"=\"*50)\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df = preprocessor.load_data()\n",
        "    df = preprocessor.explore_data(df)\n",
        "    X_train, X_test, y_train, y_test, feature_names = preprocessor.preprocess_data(df)\n",
        "\n",
        "    # Step 2: Baseline Model Training\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 2: Baseline Model Training\")\n",
        "    print(\"=\"*50)\n",
        "    trainer = ModelTrainer(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    # Train multiple baseline models\n",
        "    rf_metrics, rf_model = trainer.train_random_forest(\"baseline\")\n",
        "    gb_metrics, gb_model = trainer.train_gradient_boosting(\"baseline\")\n",
        "    lr_metrics, lr_model = trainer.train_logistic_regression(\"baseline\")\n",
        "\n",
        "    # Step 3: Hyperparameter Tuning\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 3: Hyperparameter Tuning\")\n",
        "    print(\"=\"*50)\n",
        "    tuner = HyperparameterTuner(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    rf_tuned_metrics, rf_tuned_model, rf_best_params = tuner.tune_random_forest()\n",
        "    gb_tuned_metrics, gb_tuned_model, gb_best_params = tuner.tune_gradient_boosting()\n",
        "\n",
        "    # Step 4: Model Selection\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 4: Model Selection\")\n",
        "    print(\"=\"*50)\n",
        "    selector = ModelSelector()\n",
        "    best_run = selector.select_best_model()\n",
        "\n",
        "    if best_run is not None:\n",
        "        best_run_id = best_run['run_id']\n",
        "        best_model_name = best_run['run_name']\n",
        "\n",
        "        # Step 5: Model Registration\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"STEP 5: Model Registration\")\n",
        "        print(\"=\"*50)\n",
        "        registry_manager = ModelRegistryManager()\n",
        "\n",
        "        # Register the best model\n",
        "        model_version = registry_manager.register_best_model(best_run_id)\n",
        "\n",
        "        if model_version:\n",
        "            # Transition to Staging\n",
        "            registry_manager.transition_model_stage(\n",
        "                model_version.name,\n",
        "                model_version.version,\n",
        "                \"Staging\"\n",
        "            )\n",
        "\n",
        "            # Demonstrate stage transition (Staging → Production)\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Demonstrating Stage Transition: Staging → Production\")\n",
        "            print(\"=\"*50)\n",
        "            registry_manager.transition_model_stage(\n",
        "                model_version.name,\n",
        "                model_version.version,\n",
        "                \"Production\"\n",
        "            )\n",
        "\n",
        "        # List all registered models\n",
        "        registry_manager.list_registered_models()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎉 ASSIGNMENT COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Start MLflow UI: mlflow ui\")\n",
        "    print(\"2. Open http://localhost:5000 in your browser\")\n",
        "    print(\"3. Explore experiments and model registry\")\n",
        "    print(\"4. Check the mlruns directory for artifacts\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "vH5mmFPn8iTT",
        "outputId": "af097186-6a09-4477-cf02-f3f1ff1f9cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data_preparation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3817832058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_preparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperparameterTuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_preparation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9030714"
      },
      "source": [
        "!python mlflow-assignment-1/main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02395ba0",
        "outputId": "331968ff-a096-4307-dbcc-9a52a3e27ecb"
      },
      "source": [
        "%%writefile mlflow-assignment-1/data_preparation.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and prepare the wine quality dataset\"\"\"\n",
        "        print(\"Loading Wine Quality Dataset...\")\n",
        "\n",
        "        # Method 1: Using fetch_openml (recommended)\n",
        "        try:\n",
        "            wine = fetch_openml(name='wine-quality-red', version=1, as_frame=True)\n",
        "            df = wine.frame\n",
        "            df['quality'] = df['quality'].astype(int)\n",
        "        except:\n",
        "            # Method 2: Download from URL\n",
        "            url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "            df = pd.read_csv(url, delimiter=';')\n",
        "\n",
        "        print(f\"Dataset loaded with {df.shape[0]} samples and {df.shape[1]} features\")\n",
        "        return df\n",
        "\n",
        "    def explore_data(self, df):\n",
        "        \"\"\"Perform basic data exploration\"\"\"\n",
        "        print(\"\\n=== Data Exploration ===\")\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(f\"\\nColumn types:\\n{df.dtypes}\")\n",
        "        print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "        print(f\"\\nDataset description:\\n{df.describe()}\")\n",
        "\n",
        "        # Check target distribution\n",
        "        print(f\"\\nTarget distribution (quality):\\n{df['quality'].value_counts().sort_index()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"Preprocess the data for modeling\"\"\"\n",
        "        print(\"\\n=== Data Preprocessing ===\")\n",
        "\n",
        "        # Create a binary classification problem (good wine vs bad wine)\n",
        "        df['wine_quality'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
        "\n",
        "        # Features and target\n",
        "        X = df.drop(['quality', 'wine_quality'], axis=1)\n",
        "        y = df['wine_quality']\n",
        "\n",
        "        print(f\"Features: {X.columns.tolist()}\")\n",
        "        print(f\"Target distribution: {y.value_counts()}\")\n",
        "        print(f\"Positive class ratio: {y.mean():.3f}\")\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Scale numerical features\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        print(f\"Training set: {X_train_scaled.shape}\")\n",
        "        print(f\"Test set: {X_test_scaled.shape}\")\n",
        "\n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df = preprocessor.load_data()\n",
        "    df = preprocessor.explore_data(df)\n",
        "    X_train, X_test, y_train, y_test, feature_names = preprocessor.preprocess_data(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/data_preparation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aeb923a",
        "outputId": "9ca9a882-90ed-4de6-ac04-699063f73ae3"
      },
      "source": [
        "%%writefile mlflow-assignment-1/mlflow_setup.py\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import tempfile\n",
        "\n",
        "class MLflowSetup:\n",
        "    def __init__(self, experiment_name=\"Wine-Quality-Classification\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.setup_tracking()\n",
        "\n",
        "    def setup_tracking(self):\n",
        "        \"\"\"Setup MLflow tracking with multiple backend options\"\"\"\n",
        "\n",
        "        # Option 1: Local File System (Default)\n",
        "        tracking_uri = \"file:///./mlruns\"\n",
        "\n",
        "        # Option 2: SQLite Backend (Uncomment to use)\n",
        "        # tracking_uri = \"sqlite:///mlflow.db\"\n",
        "\n",
        "        # Option 3: Remote Server (Uncomment and modify for your setup)\n",
        "        # tracking_uri = \"http://your-mlflow-server:5000\"\n",
        "\n",
        "        # Option 4: AWS S3 Backend (Uncomment for AWS deployment)\n",
        "        # tracking_uri = \"file:///./mlruns\"\n",
        "        # os.environ['AWS_ACCESS_KEY_ID'] = 'your-access-key'\n",
        "        # os.environ['AWS_SECRET_ACCESS_KEY'] = 'your-secret-key'\n",
        "        # os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https://s3.amazonaws.com'\n",
        "\n",
        "        mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "        # Set experiment\n",
        "        mlflow.set_experiment(self.experiment_name)\n",
        "\n",
        "        print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "        print(f\"Experiment: {self.experiment_name}\")\n",
        "\n",
        "    def create_run_name(self, model_type, run_description):\n",
        "        \"\"\"Create meaningful run names\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        return f\"{model_type}_{timestamp}_{run_description}\"\n",
        "\n",
        "# Initialize MLflow setup\n",
        "mlflow_setup = MLflowSetup()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/mlflow_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47b15f86",
        "outputId": "fe23a63d-cc99-468a-8789-2966af499fb7"
      },
      "source": [
        "%%writefile mlflow-assignment-1/model_training.py\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import json\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from mlflow_setup import MLflowSetup # Import MLflowSetup\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test, feature_names):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "\n",
        "    def evaluate_model(self, model, model_type):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        # Predictions\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_pred_proba = model.predict_proba(self.X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "            'precision': precision_score(self.y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(self.y_test, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(self.y_test, y_pred, zero_division=0),\n",
        "        }\n",
        "\n",
        "        if y_pred_proba is not None:\n",
        "            metrics['roc_auc'] = roc_auc_score(self.y_test, y_pred_proba)\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='f1')\n",
        "        metrics['cv_f1_mean'] = cv_scores.mean()\n",
        "        metrics['cv_f1_std'] = cv_scores.std()\n",
        "\n",
        "        return metrics, y_pred, y_pred_proba\n",
        "\n",
        "    def create_plots(self, model, y_pred, y_pred_proba, model_type):\n",
        "        \"\"\"Create evaluation plots\"\"\"\n",
        "        plots = {}\n",
        "\n",
        "        # Confusion Matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(self.y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix - {model_type}')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "\n",
        "        cm_path = tempfile.mktemp(suffix='_cm.png')\n",
        "        plt.savefig(cm_path)\n",
        "        plt.close()\n",
        "        plots['confusion_matrix'] = cm_path\n",
        "\n",
        "        # Feature Importance (for tree-based models)\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            feature_imp = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            sns.barplot(data=feature_imp.head(10), x='importance', y='feature')\n",
        "            plt.title(f'Feature Importance - {model_type}')\n",
        "\n",
        "            fi_path = tempfile.mktemp(suffix='_feature_importance.png')\n",
        "            plt.savefig(fi_path)\n",
        "            plt.close()\n",
        "            plots['feature_importance'] = fi_path\n",
        "\n",
        "        return plots\n",
        "\n",
        "    def train_random_forest(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Random Forest Classifier with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"RandomForest\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 10,\n",
        "                'min_samples_split': 2,\n",
        "                'min_samples_leaf': 1,\n",
        "                'random_state': 42\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Log dataset info\n",
        "            mlflow.log_param(\"dataset_shape\", f\"{self.X_train.shape}\")\n",
        "            mlflow.log_param(\"feature_count\", len(self.feature_names))\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Random Forest...\")\n",
        "            model = RandomForestClassifier(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"RandomForest\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Create and log plots\n",
        "            plots = self.create_plots(model, y_pred, y_pred_proba, \"RandomForest\")\n",
        "            for plot_name, plot_path in plots.items():\n",
        "                mlflow.log_artifact(plot_path, \"plots\")\n",
        "                os.remove(plot_path)  # Clean up temp file\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"random_forest_model\",\n",
        "                registered_model_name=\"RandomForest_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            # Log feature names as artifact\n",
        "            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
        "                json.dump({'feature_names': self.feature_names.tolist()}, f)\n",
        "                mlflow.log_artifact(f.name, \"metadata\")\n",
        "                os.remove(f.name)\n",
        "\n",
        "            print(f\"Random Forest completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model\n",
        "\n",
        "    def train_gradient_boosting(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Gradient Boosting Classifier with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"GradientBoosting\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'n_estimators': 100,\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 3,\n",
        "                'random_state': 42\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Gradient Boosting...\")\n",
        "            model = GradientBoostingClassifier(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"GradientBoosting\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Create and log plots\n",
        "            plots = self.create_plots(model, y_pred, y_pred_proba, \"GradientBoosting\")\n",
        "            for plot_name, plot_path in plots.items():\n",
        "                mlflow.log_artifact(plot_path, \"plots\")\n",
        "                os.remove(plot_path)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"gradient_boosting_model\",\n",
        "                registered_model_name=\"GradientBoosting_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            print(f\"Gradient Boosting completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model\n",
        "\n",
        "    def train_logistic_regression(self, run_description=\"baseline\"):\n",
        "        \"\"\"Train Logistic Regression with MLflow tracking\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"LogisticRegression\", run_description)\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameters\n",
        "            params = {\n",
        "                'C': 1.0,\n",
        "                'max_iter': 1000,\n",
        "                'random_state': 42,\n",
        "                'solver': 'liblinear'\n",
        "            }\n",
        "\n",
        "            # Log parameters\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "\n",
        "            # Train model\n",
        "            print(\"Training Logistic Regression...\")\n",
        "            model = LogisticRegression(**params)\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            metrics, y_pred, y_pred_proba = self.evaluate_model(model, \"LogisticRegression\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                model,\n",
        "                \"logistic_regression_model\",\n",
        "                registered_model_name=\"LogisticRegression_Wine_Quality\"\n",
        "            )\n",
        "\n",
        "            print(f\"Logistic Regression completed - F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/model_training.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "561421f8",
        "outputId": "f4434a9f-de41-456b-96f0-4fae51f54677"
      },
      "source": [
        "%%writefile mlflow-assignment-1/hyperparameter_tuning.py\n",
        "import mlflow\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import pandas as pd\n",
        "from mlflow_setup import MLflowSetup # Import MLflowSetup\n",
        "from model_training import ModelTrainer # Import ModelTrainer\n",
        "\n",
        "class HyperparameterTuner:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test, feature_names):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "        self.trainer = ModelTrainer(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    def tune_random_forest(self):\n",
        "        \"\"\"Perform hyperparameter tuning for Random Forest\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"RandomForest\", \"hyperparameter_tuning\")\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameter grid\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [5, 10, 15, None],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "\n",
        "            # Log tuning parameters\n",
        "            mlflow.log_param(\"tuning_method\", \"GridSearchCV\")\n",
        "            mlflow.log_param(\"param_grid\", str(param_grid))\n",
        "            mlflow.log_param(\"cv_folds\", 5)\n",
        "\n",
        "            # Perform grid search\n",
        "            print(\"Performing GridSearch for Random Forest...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                RandomForestClassifier(random_state=42),\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='f1',\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Log best parameters\n",
        "            best_params = grid_search.best_params_\n",
        "            for param, value in best_params.items():\n",
        "                mlflow.log_param(f\"best_{param}\", value)\n",
        "\n",
        "            mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            best_model = grid_search.best_estimator_\n",
        "            metrics, y_pred, y_pred_proba = self.trainer.evaluate_model(best_model, \"RandomForest_Tuned\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                best_model,\n",
        "                \"random_forest_tuned_model\",\n",
        "                registered_model_name=\"RandomForest_Wine_Quality_Tuned\"\n",
        "            )\n",
        "\n",
        "            print(f\"Random Forest Tuning completed - Best F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, best_model, grid_search.best_params_\n",
        "\n",
        "    def tune_gradient_boosting(self):\n",
        "        \"\"\"Perform hyperparameter tuning for Gradient Boosting\"\"\"\n",
        "        run_name = self.mlflow_setup.create_run_name(\"GradientBoosting\", \"hyperparameter_tuning\")\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Define parameter grid\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 4, 5],\n",
        "                'subsample': [0.8, 0.9, 1.0]\n",
        "            }\n",
        "\n",
        "            # Log tuning parameters\n",
        "            mlflow.log_param(\"tuning_method\", \"GridSearchCV\")\n",
        "            mlflow.log_param(\"param_grid\", str(param_grid))\n",
        "\n",
        "            # Perform grid search\n",
        "            print(\"Performing GridSearch for Gradient Boosting...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                GradientBoostingClassifier(random_state=42),\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='f1',\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Log best parameters\n",
        "            best_params = grid_search.best_params_\n",
        "            for param, value in best_params.items():\n",
        "                mlflow.log_param(f\"best_{param}\", value)\n",
        "\n",
        "            mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            best_model = grid_search.best_estimator_\n",
        "            metrics, y_pred, y_pred_proba = self.trainer.evaluate_model(best_model, \"GradientBoosting_Tuned\")\n",
        "\n",
        "            # Log metrics\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(metric, value)\n",
        "\n",
        "            # Log model\n",
        "            mlflow.sklearn.log_model(\n",
        "                best_model,\n",
        "                \"gradient_boosting_tuned_model\",\n",
        "                registered_model_name=\"GradientBoosting_Wine_Quality_Tuned\"\n",
        "            )\n",
        "\n",
        "            print(f\"Gradient Boosting Tuning completed - Best F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "            return metrics, best_model, grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/hyperparameter_tuning.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f643be0",
        "outputId": "8f06a05e-84dd-4f6e-833a-f334b1e737f8"
      },
      "source": [
        "%%writefile mlflow-assignment-1/model_selection.py\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlflow_setup import MLflowSetup # Import MLflowSetup\n",
        "\n",
        "class ModelSelector:\n",
        "    def __init__(self):\n",
        "        self.client = MlflowClient()\n",
        "        self.mlflow_setup = MLflowSetup()\n",
        "\n",
        "    def get_all_runs(self):\n",
        "        \"\"\"Retrieve all runs from the current experiment\"\"\"\n",
        "        experiment = mlflow.get_experiment_by_name(self.mlflow_setup.experiment_name)\n",
        "        runs = self.client.search_runs(experiment.experiment_id)\n",
        "\n",
        "        run_data = []\n",
        "        for run in runs:\n",
        "            run_info = {\n",
        "                'run_id': run.info.run_id,\n",
        "                'run_name': run.data.tags.get('mlflow.runName', ''),\n",
        "                'model_type': run.data.tags.get('mlflow.runName', '').split('_')[0],\n",
        "                'status': run.info.status,\n",
        "                'start_time': run.info.start_time,\n",
        "            }\n",
        "\n",
        "            # Add parameters\n",
        "            for key, value in run.data.params.items():\n",
        "                run_info[f'param_{key}'] = value\n",
        "\n",
        "            # Add metrics\n",
        "            for key, value in run.data.metrics.items():\n",
        "                run_info[f'metric_{key}'] = value\n",
        "\n",
        "            run_data.append(run_info)\n",
        "\n",
        "        return pd.DataFrame(run_data)\n",
        "\n",
        "    def select_best_model(self, primary_metric='metric_f1_score', secondary_metric='metric_roc_auc'):\n",
        "        \"\"\"Select the best model based on specified metrics\"\"\"\n",
        "        runs_df = self.get_all_runs()\n",
        "\n",
        "        if runs_df.empty:\n",
        "            print(\"No runs found in the experiment\")\n",
        "            return None\n",
        "\n",
        "        # Filter completed runs\n",
        "        completed_runs = runs_df[runs_df['status'] == 'FINISHED']\n",
        "\n",
        "        if completed_runs.empty:\n",
        "            print(\"No completed runs found\")\n",
        "            return None\n",
        "\n",
        "        # Sort by primary and secondary metrics\n",
        "        best_run = completed_runs.sort_values(\n",
        "            [primary_metric, secondary_metric],\n",
        "            ascending=[False, False]\n",
        "        ).iloc[0]\n",
        "\n",
        "        print(\"=== BEST MODEL SELECTION RESULTS ===\")\n",
        "        print(f\"Best Run ID: {best_run['run_id']}\")\n",
        "        print(f\"Best Run Name: {best_run['run_name']}\")\n",
        "        print(f\"Primary Metric ({primary_metric}): {best_run[primary_metric]:.4f}\")\n",
        "        print(f\"Secondary Metric ({secondary_metric}): {best_run.get(secondary_metric, 'N/A')}\")\n",
        "\n",
        "        # Display comparison table\n",
        "        comparison_df = completed_runs.groupby('model_type').agg({\n",
        "            'metric_accuracy': 'mean',\n",
        "            'metric_precision': 'mean',\n",
        "            'metric_recall': 'mean',\n",
        "            'metric_f1_score': 'mean',\n",
        "            'metric_roc_auc': 'mean'\n",
        "        }).round(4)\n",
        "\n",
        "        print(\"\\n=== MODEL COMPARISON ===\")\n",
        "        print(comparison_df)\n",
        "\n",
        "        return best_run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/model_selection.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec7a0136",
        "outputId": "c3dcc287-1c38-47d6-c3bc-765a674efeec"
      },
      "source": [
        "%%writefile mlflow-assignment-1/model_registry.py\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import time\n",
        "\n",
        "class ModelRegistryManager:\n",
        "    def __init__(self):\n",
        "        self.client = MlflowClient()\n",
        "\n",
        "    def register_best_model(self, run_id, model_name=\"Best_Wine_Quality_Model\"):\n",
        "        \"\"\"Register the best model in MLflow Model Registry\"\"\"\n",
        "\n",
        "        # Construct model URI\n",
        "        model_uri = f\"runs:/{run_id}/model\"\n",
        "\n",
        "        try:\n",
        "            # Register the model\n",
        "            print(f\"Registering model from run {run_id}...\")\n",
        "            mv = mlflow.register_model(model_uri, model_name)\n",
        "\n",
        "            print(f\"Model registered successfully!\")\n",
        "            print(f\"Model Name: {mv.name}\")\n",
        "            print(f\"Model Version: {mv.version}\")\n",
        "            print(f\"Current Stage: {mv.current_stage}\")\n",
        "\n",
        "            return mv\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error registering model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def transition_model_stage(self, model_name, version, stage):\n",
        "        \"\"\"Transition model to different stages (Staging → Production)\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Transition model stage\n",
        "            self.client.transition_model_version_stage(\n",
        "                name=model_name,\n",
        "                version=version,\n",
        "                stage=stage\n",
        "            )\n",
        "\n",
        "            print(f\"Model {model_name} version {version} transitioned to {stage} stage\")\n",
        "\n",
        "            # Wait for transition to complete\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Get updated model version\n",
        "            mv = self.client.get_model_version(model_name, version)\n",
        "            print(f\"Current stage: {mv.current_stage}\")\n",
        "\n",
        "            return mv\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transitioning model stage: {e}\")\n",
        "            return None\n",
        "\n",
        "    def list_registered_models(self):\n",
        "        \"\"\"List all registered models\"\"\"\n",
        "        models = self.client.search_registered_models()\n",
        "\n",
        "        print(\"=== REGISTERED MODELS ===\")\n",
        "        for model in models:\n",
        "            print(f\"Model: {model.name}\")\n",
        "            for version in model.latest_versions:\n",
        "                print(f\"  Version {version.version}: {version.current_stage}\")\n",
        "\n",
        "    def archive_old_versions(self, model_name, keep_versions=3):\n",
        "        \"\"\"Archive old model versions to keep registry clean\"\"\"\n",
        "        try:\n",
        "            # Get all versions\n",
        "            versions = self.client.search_model_versions(f\"name='{model_name}'\")\n",
        "\n",
        "            # Sort by version number and get old versions\n",
        "            sorted_versions = sorted(versions, key=lambda x: x.version, reverse=True)\n",
        "            old_versions = sorted_versions[keep_versions:]\n",
        "\n",
        "            for version in old_versions:\n",
        "                if version.current_stage == \"None\":\n",
        "                    self.client.transition_model_version_stage(\n",
        "                        name=model_name,\n",
        "                        version=version.version,\n",
        "                        stage=\"Archived\"\n",
        "                    )\n",
        "                    print(f\"Archived version {version.version}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error archiving old versions: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/model_registry.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2cad2e",
        "outputId": "76ebd0af-3a93-4f40-d1b7-37b24020f444"
      },
      "source": [
        "%%writefile mlflow-assignment-1/main.py\n",
        "import mlflow\n",
        "from data_preparation import DataPreprocessor\n",
        "from model_training import ModelTrainer\n",
        "from hyperparameter_tuning import HyperparameterTuner\n",
        "from model_selection import ModelSelector\n",
        "from model_registry import ModelRegistryManager\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    print(\"🚀 Starting MLOps Assignment 1 - MLflow Experiment Tracking\")\n",
        "\n",
        "    # Step 1: Data Preparation\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: Data Preparation\")\n",
        "    print(\"=\"*50)\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df = preprocessor.load_data()\n",
        "    df = preprocessor.explore_data(df)\n",
        "    X_train, X_test, y_train, y_test, feature_names = preprocessor.preprocess_data(df)\n",
        "\n",
        "    # Step 2: Baseline Model Training\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 2: Baseline Model Training\")\n",
        "    print(\"=\"*50)\n",
        "    trainer = ModelTrainer(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    # Train multiple baseline models\n",
        "    rf_metrics, rf_model = trainer.train_random_forest(\"baseline\")\n",
        "    gb_metrics, gb_model = trainer.train_gradient_boosting(\"baseline\")\n",
        "    lr_metrics, lr_model = trainer.train_logistic_regression(\"baseline\")\n",
        "\n",
        "    # Step 3: Hyperparameter Tuning\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 3: Hyperparameter Tuning\")\n",
        "    print(\"=\"*50)\n",
        "    tuner = HyperparameterTuner(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    rf_tuned_metrics, rf_tuned_model, rf_best_params = tuner.tune_random_forest()\n",
        "    gb_tuned_metrics, gb_tuned_model, gb_best_params = tuner.tune_gradient_boosting()\n",
        "\n",
        "    # Step 4: Model Selection\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 4: Model Selection\")\n",
        "    print(\"=\"*50)\n",
        "    selector = ModelSelector()\n",
        "    best_run = selector.select_best_model()\n",
        "\n",
        "    if best_run is not None:\n",
        "        best_run_id = best_run['run_id']\n",
        "        best_model_name = best_run['run_name']\n",
        "\n",
        "        # Step 5: Model Registration\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"STEP 5: Model Registration\")\n",
        "        print(\"=\"*50)\n",
        "        registry_manager = ModelRegistryManager()\n",
        "\n",
        "        # Register the best model\n",
        "        model_version = registry_manager.register_best_model(best_run_id)\n",
        "\n",
        "        if model_version:\n",
        "            # Transition to Staging\n",
        "            registry_manager.transition_model_stage(\n",
        "                model_version.name,\n",
        "                model_version.version,\n",
        "                \"Staging\"\n",
        "            )\n",
        "\n",
        "            # Demonstrate stage transition (Staging → Production)\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Demonstrating Stage Transition: Staging → Production\")\n",
        "            print(\"=\"*50)\n",
        "            registry_manager.transition_model_stage(\n",
        "                model_version.name,\n",
        "                model_version.version,\n",
        "                \"Production\"\n",
        "            )\n",
        "\n",
        "        # List all registered models\n",
        "        registry_manager.list_registered_models()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎉 ASSIGNMENT COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Start MLflow UI: mlflow ui\")\n",
        "    print(\"2. Open http://localhost:5000 in your browser\")\n",
        "    print(\"3. Explore experiments and model registry\")\n",
        "    print(\"4. Check the mlruns directory for artifacts\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlflow-assignment-1/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddTn5Gmb8nB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}